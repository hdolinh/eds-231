---
title: 'Topic 3: Sentiment Analysis I'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  tidyr,
  # text analysis in R
  lubridate,
  pdftools,
  # read in pdfs
  tidyverse,
  tidytext,
  here,
  LexisNexisTools,
  # Nexis Uni data wrangling
  sentimentr,
  readr,
  readtext
)
```

# Sentiment Anaysis I

From the [Nexis Uni database](https://www.library.ucsb.edu/research/db/211), I chose the key search term "carbon management". *Note: when I used `lnt_read` I was prompted that more than one language was detected and I chose English only.*


```{r}
my_files <- list.files(pattern = ".docx",
                       path = here("hw/data"),
                       full.names = TRUE,
                       recursive = TRUE,
                       ignore.case = TRUE)

data <- lnt_read(my_files_test,
                 exclude_lines = "^LOAD-DATE: |^UPDATE: |^GRAFIK: |^GRAPHIC: |^DATELINE: |^Graphic |^CONTACT: |^Contact |^Email")
```

Here I am extract the metadata, articles, and paragraphs from the LNT object and converting them into data frames. **Note I received a warning that `data.frame()` was deprecated and to use `tibble()` instead.**

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- tibble(element_id = seq(1:length(meta_df$Headline)), 
                data = meta_df$Date,
                headline = meta_df$Headline)

# use if necessary
paragraphs_data <- tibble(element_id = paragraphs_df$Art_ID,
                          text = paragraphs_df$Paragraph)

data3 <- inner_join(data2, paragraphs_data, by = "element_id")
```


Here I am doing some cleaning to the text.

```{r}
data3_clean <- data3 %>% 
  # remove last email value
  mutate(text = str_remove_all(string = text,
                               pattern = "email")) %>% 
  # remove remaining rows with "contact"
  mutate(text = str_remove_all(string = text,
                               pattern = "Contact")) %>%
  # remove websites   
  mutate(text = str_remove_all(string = text,
                               pattern = "\\S*.com\\S*")) %>%
  # remove dashes at the start of a string
  mutate(text = str_remove_all(string = text,
                                pattern = "^-")) %>% 
  # remove punctuation
  mutate(text = str_remove_all(string = text,
                               pattern = "[:punct:]")) %>% 
  # remove symbols (specifically +)
  mutate(text = str_remove_all(string = text,
                               pattern = "[:symbol:].\\s")) %>% 
  # remove phone numbers
  mutate(text = str_remove_all(string = text,
                               pattern = "[:digit:]{10,}"))

# remove values that contain tags
data3_clean <- data3_clean[!grepl("Tags", data3_clean$text),]

# remove blank values
tib_clean <- subset(data3_clean, text != "")
data3_clean <- tib_clean

# Note(HD): \\S* match any number of non-space characters
```


Here I am pulling out the sentiment from the text.

```{r}
my_text <- get_sentences(data3_clean$text)
sent <- sentiment(my_text)

sent_df <- inner_join(data3_clean, sent, by = "element_id")

sentiment <- sentiment_by(sent_df$text)

sent_df %>% arrange(sentiment)
```




