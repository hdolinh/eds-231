---
title: 'Topic 3: Sentiment Analysis I'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  tidyr, # text analysis in R
  lubridate,
  pdftools, # read in pdfs
  tidyverse,
  tidytext,
  here,
  LexisNexisTools, # Nexis Uni data wrangling
  sentimentr,
  readr,
  readtext
)
```

# Sentiment Anaysis I

From the [Nexis Uni database](https://www.library.ucsb.edu/research/db/211), I chose the key search term "carbon management". *Note: when I used `lnt_read` I was prompted that more than one language was detected and I chose English only.*

```{r}
# reading in 100 text results for carbon management
path <- ("/Users/hdolinh/MEDS/EDS231/eds-231/hw/data/")

my_files <- list.files(pattern = ".docx",
                       path = path,
                       full.names = TRUE,
                       recursive = TRUE,
                       ignore.case = TRUE)

data <- lnt_read(my_files) # object of class "LNT output"
```

Here I am extract the metadata, articles, and paragraphs from the LNT object and converting them into data frames. **Note I receieved a warning that `data.frame()` was deprecated and to use `tibble()` instead.**

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- tibble(element_id = seq(1:length(meta_df$Headline)), 
                    data = meta_df$Date, 
                    headline = meta_df$Headline)

# use if necessary
paragraphs_data <- tibble(element_id = paragraphs_df$Art_ID,
                          text = paragraphs_df$Paragraph)

data3 <- inner_join(data2, paragraphs_data, by = "element_id")
```


Here I am doing some cleaning to the text.

```{r}
data3_clean <- data3 %>% 
  # remove emails
  mutate(text = str_remove_all(string = text,
                               pattern = "\\S*@\\S*")) %>% # \\S* match any number of non-space characters
  # remove websites   
  mutate(text = str_remove_all(string = text,
                               pattern = "\\S*.com\\S*")) %>% 
  # remove dashes at the start of a string
  mutate(text = str_remove_all(string = text,
                                pattern = "^-")) %>% 
  # remove punctuation
  mutate(text = str_remove_all(string = text,
                               pattern = "[:punct:]")) %>% 
  # remove phone numbers
  mutate(text = str_remove_all(string = text,
                               pattern = "[:digit:]{10,}"))

# remove values that contain tags
data3_clean <- data3_clean[!grepl("Tags", data3_clean$text),]

# remove values with contact information
data3_clean <- data3_clean[!grepl("Contact", data3_clean$text),]

# remove blank values
tib_clean <- subset(data3_clean, text != "")
data3_clean <- tib_clean
```


Here I am pulling out the sentiment from the text.

```{r}
my_text <- get_sentences(data3_clean$text)
sent <- sentiment(my_text)

sent_df <- inner_join(data3_clean, sent, by = "element_id")

sentiment <- sentiment_by(sent_df$text)

sent_df %>% arrange(sentiment)
```




