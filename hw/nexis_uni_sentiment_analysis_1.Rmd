---
title: 'Topic 3: Sentiment Analysis I'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  tidyr,
  # text analysis in R
  lubridate,
  pdftools,
  # read in pdfs
  tidyverse,
  tidytext,
  here,
  LexisNexisTools,
  # Nexis Uni data wrangling
  sentimentr,
  readr,
  readtext
)
```

# Sentiment Anaysis I

0. Using the Intergovernmental Panel on Climate Change (IPCC) Nexis Uni dataset, I was able to recreate Figure 1A from [Public Perceptions of Aquaculture: Evaluating Spatiotemporal Patterns of Sentiment around the World, Froehlich et al](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169281). 

First, I read in the data and created a data frame that contains the necessary data I need: `element_id`, `date`, and `headline`.

```{r}
# downloaded data from eds 231 github
# don't have to specify file in here() bc list.files automatically looks 
# for the files based on the pattern
ipcc_files <- list.files(pattern = ".docx", path = here("hw/ipcc_data"),
                         full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

ipcc_data <- lnt_read(ipcc_files) # class 'LNToutput'

# pull out necessary data
meta_ipcc <- ipcc_data@meta
articles_ipcc <- ipcc_data@articles
paragraphs_ipcc <- ipcc_data@paragraphs

# create tibble
ipcc_data2 <- tibble(element_id = seq(1:length(meta_ipcc$Headline)), 
                     date = meta_ipcc$Date, 
                     headline = meta_ipcc$Headline)
```

Next I obtain all the headline sentences using `get_sentences()`, which resulted in a total of 100 lists. I then use `sentiment()` to approximate a polarity score of each sentence ranging from -1 to 1, or negative to positive. I join the data frame of headlines I created in the code chunk above with the data frame of sentiment polarity scores. My joined data frame has a length of 109 because some headlines are made up of multiple sentences. 

`sentiment_by` provides a data frame that has estimated the sentiment grouped by `headline`.

```{r, results='hide'}
ipcc_text <- get_sentences(ipcc_data2$headline)
sent_ipcc <- sentiment(ipcc_text)

sent_ipcc_df <- inner_join(ipcc_data2, sent_ipcc, by = "element_id")

sentiment_ipcc <- sentiment_by(sent_ipcc_df$headline)

sent_ipcc_df %>% arrange(sentiment_ipcc)
```

Here I am assigning each polarity score a sentiment category of Positive, Neutral, or Negative. Then I grouped by date and sentiment category to find the total counts of each sentiment category by date. I need these categories to recreate the Froehlich et al paper.

```{r, results='hide'}
sent_ipcc_aggregate <- sent_ipcc_df %>% 
  mutate(sentiment_category = case_when(sentiment > 0 ~ "Positive",
                                        sentiment == 0 ~ "Neutral",
                                        sentiment < 0 ~ "Negative")) %>% 
  group_by(date, sentiment_category) %>% 
  summarize(count = n())
```

Now that I have my data in the right format, I can recreate the plot.

```{r}
ggplot(data = sent_ipcc_aggregate, 
       aes(x = date, y = count, color = sentiment_category)) +
  geom_line(size = 1.5) +
  theme_minimal() +
  theme(legend.position = c(0.8, 0.7)) +
  scale_x_date(date_labels = "%Y-%m-%d",
               breaks = unique(sent_ipcc_aggregate$date)) +
  labs(title = "Sentiment of IPCC Articles from Nexis Uni database (2022/04/04 - 2022/04/11)",
       x = "Date",
       y = "Number of headlines",
       color = "Sentiment Category")
```


From the [Nexis Uni database](https://www.library.ucsb.edu/research/db/211), I chose the key search term "carbon management". *Note: when I used `lnt_read` I was prompted that more than one language was detected and I chose English only.*


```{r}
my_files <- list.files(pattern = ".docx",
                       path = here("hw/data"),
                       full.names = TRUE,
                       recursive = TRUE,
                       ignore.case = TRUE)

data <- lnt_read(my_files,
                 exclude_lines = "^LOAD-DATE: |^UPDATE: |^GRAFIK: |^GRAPHIC: |^DATELINE: |^Graphic |^CONTACT: |^Contact |^Email")
```

Here I am extract the metadata, articles, and paragraphs from the LNT object and converting them into data frames. **Note I received a warning that `data.frame()` was deprecated and to use `tibble()` instead.**

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- tibble(element_id = seq(1:length(meta_df$Headline)), 
                data = meta_df$Date,
                headline = meta_df$Headline)

# use if necessary
paragraphs_data <- tibble(element_id = paragraphs_df$Art_ID,
                          text = paragraphs_df$Paragraph)

data3 <- inner_join(data2, paragraphs_data, by = "element_id")
```


Here I am doing some cleaning to the text.

```{r}
data3_clean <- data3 %>% 
  # remove last email value
  mutate(text = str_remove_all(string = text,
                               pattern = "email")) %>% 
  # remove remaining rows with "contact"
  mutate(text = str_remove_all(string = text,
                               pattern = "Contact")) %>%
  # remove websites   
  mutate(text = str_remove_all(string = text,
                               pattern = "\\S*.com\\S*")) %>%
  # remove dashes at the start of a string
  mutate(text = str_remove_all(string = text,
                                pattern = "^-")) %>% 
  # remove punctuation
  mutate(text = str_remove_all(string = text,
                               pattern = "[:punct:]")) %>% 
  # remove symbols (specifically +)
  mutate(text = str_remove_all(string = text,
                               pattern = "[:symbol:].\\s")) %>% 
  # remove phone numbers
  mutate(text = str_remove_all(string = text,
                               pattern = "[:digit:]{10,}"))

# remove values that contain tags
data3_clean <- data3_clean[!grepl("Tags", data3_clean$text),]

# remove blank values
tib_clean <- subset(data3_clean, text != "")
data3_clean <- tib_clean

# Note(HD): \\S* match any number of non-space characters
```

Bing sentiment
```{r}
bing_sent <- get_sentiments('bing') 
```

stop words
```{r}
custom_stop_words <- bind_rows(tibble(word = c("your_word"),  
                                      lexicon = c("custom")), 
                               stop_words)
```


unnest
```{r}
#unnest to word-level tokens, remove stop words, and join sentiment words
 text_words <- data3_clean  %>%
  unnest_tokens(output = word, input = text, token = 'words')
 
 sent_words <- text_words %>% #break text into individual words
  anti_join(custom_stop_words, by = 'word') %>% #returns only the rows without stop words
  inner_join(bing_sent, by = 'word') #joins and retains only sentiment words
```

NRC emotion lexicon to obtain more specific sentiments
```{r}
nrc_sent <- get_sentiments('nrc') %>% 
  filter(!sentiment %in% c("positive", "negative")) #requires downloading a large dataset via prompt

# exploratory analysis
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

#most common words by sentiment
fear_words <- data3_clean  %>%
  unnest_tokens(output = word, input = text, token = 'words') %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)

nrc_word_counts <- text_words %>%
  inner_join(nrc_sent) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

join nrc word cont and text words to get dates

NOTE CHANGE FROM HEADLINE TO PARAGRAPH
```{r}
sentiment_plot <- text_words %>% 
  inner_join(nrc_sent, by = "word") %>% 
  group_by(data) %>% 
  count(sentiment) %>% 
  mutate(total_words_day = sum(n)) %>% 
  mutate(pct_sent_day = n / total_words_day)
```

plot
```{r}
ggplot(data = sentiment_plot, aes(x = data, y = pct_sent_day, color = sentiment)) +
  geom_smooth(method = "lm", se = F)
```



