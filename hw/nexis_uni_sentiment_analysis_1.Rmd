---
title: 'Topic 3: Sentiment Analysis I'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  tidyr,
  # text analysis in R
  lubridate,
  pdftools,
  # read in pdfs
  tidyverse,
  tidytext,
  here,
  LexisNexisTools,
  # Nexis Uni data wrangling
  sentimentr,
  readr,
  readtext
)
```

# Sentiment Anaysis I

IPCC

```{r}
#to follow along with this example, download this .docx to your working directory: 
#https://github.com/MaRo406/EDS_231-text-sentiment/blob/main/nexis_dat/Nexis_IPCC_Results.docx
ipcc_files <- list.files(pattern = ".docx", path = here("hw/ipcc_data"),
                       full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

ipcc_data <- lnt_read(ipcc_files) #Object of class 'LNT output'


meta_ipcc <- ipcc_data@meta
articles_ipcc <- ipcc_data@articles
paragraphs_ipcc <- ipcc_data@paragraphs

ipcc_data2 <- tibble(element_id = seq(1:length(meta_ipcc$Headline)), date = meta_ipcc$Date, headline = meta_ipcc$Headline)
```

```{r}
#can we create a similar graph to Figure 3A from Froelich et al.? 
ipcc_text <- get_sentences(ipcc_data2$headline)
sent_ipcc <- sentiment(ipcc_text)

sent_ipcc_df <- inner_join(ipcc_data2, sent_ipcc, by = "element_id")

sentiment_ipcc <- sentiment_by(sent_ipcc_df$headline)

sent_ipcc_df %>% arrange(sentiment_ipcc)
```


```{r}
sent_ipcc_aggregate <- sent_ipcc_df %>% 
  mutate(sentiment_category = case_when(sentiment > 0 ~ "Positive",
                                        sentiment == 0 ~ "Neutral",
                                        sentiment < 0 ~ "Negative")) %>% 
  group_by(date, sentiment_category) %>% 
  summarize(count = n())
```

Now I plot
```{r}
ggplot(data = sent_ipcc_aggregate, aes(x = date, y = count, color = sentiment_category)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Sentiment of IPCC Articles from Nexis Uni database from 2022/04/11 - 2022/04/11",
       x = "Date",
       y = "Count",
       color = "Sentiment Category")
```


From the [Nexis Uni database](https://www.library.ucsb.edu/research/db/211), I chose the key search term "carbon management". *Note: when I used `lnt_read` I was prompted that more than one language was detected and I chose English only.*


```{r}
my_files <- list.files(pattern = ".docx",
                       path = here("hw/data"),
                       full.names = TRUE,
                       recursive = TRUE,
                       ignore.case = TRUE)

data <- lnt_read(my_files_test,
                 exclude_lines = "^LOAD-DATE: |^UPDATE: |^GRAFIK: |^GRAPHIC: |^DATELINE: |^Graphic |^CONTACT: |^Contact |^Email")
```

Here I am extract the metadata, articles, and paragraphs from the LNT object and converting them into data frames. **Note I received a warning that `data.frame()` was deprecated and to use `tibble()` instead.**

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- tibble(element_id = seq(1:length(meta_df$Headline)), 
                data = meta_df$Date,
                headline = meta_df$Headline)

# use if necessary
paragraphs_data <- tibble(element_id = paragraphs_df$Art_ID,
                          text = paragraphs_df$Paragraph)

data3 <- inner_join(data2, paragraphs_data, by = "element_id")
```


Here I am doing some cleaning to the text.

```{r}
data3_clean <- data3 %>% 
  # remove last email value
  mutate(text = str_remove_all(string = text,
                               pattern = "email")) %>% 
  # remove remaining rows with "contact"
  mutate(text = str_remove_all(string = text,
                               pattern = "Contact")) %>%
  # remove websites   
  mutate(text = str_remove_all(string = text,
                               pattern = "\\S*.com\\S*")) %>%
  # remove dashes at the start of a string
  mutate(text = str_remove_all(string = text,
                                pattern = "^-")) %>% 
  # remove punctuation
  mutate(text = str_remove_all(string = text,
                               pattern = "[:punct:]")) %>% 
  # remove symbols (specifically +)
  mutate(text = str_remove_all(string = text,
                               pattern = "[:symbol:].\\s")) %>% 
  # remove phone numbers
  mutate(text = str_remove_all(string = text,
                               pattern = "[:digit:]{10,}"))

# remove values that contain tags
data3_clean <- data3_clean[!grepl("Tags", data3_clean$text),]

# remove blank values
tib_clean <- subset(data3_clean, text != "")
data3_clean <- tib_clean

# Note(HD): \\S* match any number of non-space characters
```


Here I am pulling out the sentiment from the text.

```{r}
my_text <- get_sentences(data3_clean$text)
sent <- sentiment(my_text)

sent_df <- inner_join(data3_clean, sent, by = "element_id")

sentiment <- sentiment_by(sent_df$text)

sent_df %>% arrange(sentiment)
```




