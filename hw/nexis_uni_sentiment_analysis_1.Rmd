---
title: 'Topic 3: Sentiment Analysis I'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  tidyr,
  # text analysis in R
  lubridate,
  pdftools,
  # read in pdfs
  tidyverse,
  tidytext,
  here,
  LexisNexisTools,
  # Nexis Uni data wrangling
  sentimentr,
  readr,
  readtext
)
```

# Sentiment Anaysis I

IPCC

```{r}
#to follow along with this example, download this .docx to your working directory: 
#https://github.com/MaRo406/EDS_231-text-sentiment/blob/main/nexis_dat/Nexis_IPCC_Results.docx
ipcc_files <- list.files(pattern = ".docx", path = here("hw/ipcc_data"),
                       full.names = TRUE, recursive = TRUE, ignore.case = TRUE)

ipcc_data <- lnt_read(ipcc_files) #Object of class 'LNT output'


meta_ipcc <- ipcc_data@meta
articles_ipcc <- ipcc_data@articles
paragraphs_ipcc <- ipcc_data@paragraphs

ipcc_data2 <- tibble(element_id = seq(1:length(meta_ipcc$Headline)), date = meta_ipcc$Date, headline = meta_ipcc$Headline)
```

```{r}
#can we create a similar graph to Figure 3A from Froelich et al.? 
ipcc_text <- get_sentences(ipcc_data2$headline)
sent_ipcc <- sentiment(ipcc_text)

sent_ipcc_df <- inner_join(ipcc_data2, sent_ipcc, by = "element_id")

sentiment_ipcc <- sentiment_by(sent_ipcc_df$headline)

sent_ipcc_df %>% arrange(sentiment_ipcc)
```
Here I am pulling out the sentiment from the text.

```{r}
sent_ipcc_aggregate <- sent_ipcc_df %>% 
  mutate(sentiment_category = case_when(sentiment > 0 ~ "Positive",
                                        sentiment == 0 ~ "Neutral",
                                        sentiment < 0 ~ "Negative")) %>% 
  group_by(date, sentiment_category) %>% 
  summarize(count = n())
```

Now I plot
```{r}
ggplot(data = sent_ipcc_aggregate, aes(x = date, y = count, color = sentiment_category)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Sentiment of IPCC Articles from Nexis Uni database from 2022/04/11 - 2022/04/11",
       x = "Date",
       y = "Count",
       color = "Sentiment Category")
```


From the [Nexis Uni database](https://www.library.ucsb.edu/research/db/211), I chose the key search term "carbon management". *Note: when I used `lnt_read` I was prompted that more than one language was detected and I chose English only.*


```{r}
my_files <- list.files(pattern = ".docx",
                       path = here("hw/data"),
                       full.names = TRUE,
                       recursive = TRUE,
                       ignore.case = TRUE)

data <- lnt_read(my_files,
                 exclude_lines = "^LOAD-DATE: |^UPDATE: |^GRAFIK: |^GRAPHIC: |^DATELINE: |^Graphic |^CONTACT: |^Contact |^Email")
```

Here I am extract the metadata, articles, and paragraphs from the LNT object and converting them into data frames. **Note I received a warning that `data.frame()` was deprecated and to use `tibble()` instead.**

```{r}
meta_df <- data@meta
articles_df <- data@articles
paragraphs_df <- data@paragraphs

data2 <- tibble(element_id = seq(1:length(meta_df$Headline)), 
                data = meta_df$Date,
                headline = meta_df$Headline)

# use if necessary
paragraphs_data <- tibble(element_id = paragraphs_df$Art_ID,
                          text = paragraphs_df$Paragraph)

data3 <- inner_join(data2, paragraphs_data, by = "element_id")
```


Here I am doing some cleaning to the text.

```{r}
data3_clean <- data3 %>% 
  # remove last email value
  mutate(text = str_remove_all(string = text,
                               pattern = "email")) %>% 
  # remove remaining rows with "contact"
  mutate(text = str_remove_all(string = text,
                               pattern = "Contact")) %>%
  # remove websites   
  mutate(text = str_remove_all(string = text,
                               pattern = "\\S*.com\\S*")) %>%
  # remove dashes at the start of a string
  mutate(text = str_remove_all(string = text,
                                pattern = "^-")) %>% 
  # remove punctuation
  mutate(text = str_remove_all(string = text,
                               pattern = "[:punct:]")) %>% 
  # remove symbols (specifically +)
  mutate(text = str_remove_all(string = text,
                               pattern = "[:symbol:].\\s")) %>% 
  # remove phone numbers
  mutate(text = str_remove_all(string = text,
                               pattern = "[:digit:]{10,}"))

# remove values that contain tags
data3_clean <- data3_clean[!grepl("Tags", data3_clean$text),]

# remove blank values
tib_clean <- subset(data3_clean, text != "")
data3_clean <- tib_clean

# Note(HD): \\S* match any number of non-space characters
```

Bing sentiment
```{r}
bing_sent <- get_sentiments('bing') 
```

stop words
```{r}
custom_stop_words <- bind_rows(tibble(word = c("your_word"),  
                                      lexicon = c("custom")), 
                               stop_words)
```


unnest
```{r}
#unnest to word-level tokens, remove stop words, and join sentiment words
 text_words <- data3_clean  %>%
  unnest_tokens(output = word, input = text, token = 'words')
 
 sent_words <- text_words %>% #break text into individual words
  anti_join(custom_stop_words, by = 'word') %>% #returns only the rows without stop words
  inner_join(bing_sent, by = 'word') #joins and retains only sentiment words
```

NRC emotion lexicon to obtain more specific sentiments
```{r}
nrc_sent <- get_sentiments('nrc') %>% 
  filter(!sentiment %in% c("positive", "negative")) #requires downloading a large dataset via prompt

# exploratory analysis
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

#most common words by sentiment
fear_words <- data3_clean  %>%
  unnest_tokens(output = word, input = text, token = 'words') %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)

nrc_word_counts <- text_words %>%
  inner_join(nrc_sent) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

join nrc word cont and text words to get dates

NOTE CHANGE FROM HEADLINE TO PARAGRAPH
```{r}
sentiment_plot <- text_words %>% 
  inner_join(nrc_sent, by = "word") %>% 
  group_by(data) %>% 
  count(sentiment) %>% 
  mutate(total_words_day = sum(n)) %>% 
  mutate(pct_sent_day = n / total_words_day)
```

plot
```{r}
ggplot(data = sentiment_plot, aes(x = data, y = pct_sent_day, color = sentiment)) +
  geom_smooth(method = "lm", se = F)
```



