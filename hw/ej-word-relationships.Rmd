---
title: 'Topic 5: Word Relationships EPA Reports on EJ'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  tidyr, # text analysis in R
  pdftools, # read in EPA pdf reports??
  lubridate,
  tidyverse,
  tidytext,
  readr,
  quanteda,
  readtext, # quanteda subpackage for reading pdf
  quanteda.textstats,
  quanteda.textplots,
  ggplot2,
  forcats,
  stringr,
  widyr, # pairwise correlations
  igraph, # network plots
  ggraph,
  here
)

# read in data
files <- list.files(path = here("hw/epa_data"),
                    pattern = "pdf$",
                    full.names = TRUE)

ej_reports <- lapply(files, pdf_text)

ej_pdf <- readtext(file = here("hw/epa_data", "*.pdf"),
                   docvarsfrom = "filenames",
                   docvarsnames = c("type", "year"),
                   sep = "_")
```


Here I am creating an initial corpus, amending stop words, and converting the data into tidy format.

```{r}
# intial corpus
epa_corpus <- corpus(x = ej_pdf,
                     text_field = "text")
summary(epa_corpus)
```

```{r}
# amending stop words
more_stops <-
  c("2015",
    "2016",
    "2017",
    "2018",
    "2019",
    "2020",
    "www.epa.gov",
    "https")
add_stops <- tibble(word = c(stop_words$word, more_stops))
# use stop vector with quanteda tools
stop_vec <- as_vector(add_stops)
```


```{r}
# tidy format
tidy_text <- tidy(epa_corpus)

# adding stop words
words <- tidy_text %>% 
  mutate(year = as.factor(docvar3)) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(add_stops,  by = 'word') %>% 
  select(-docvar3)
```

Here I am creating data objects so I can do analysis.

```{r}
# most frequent words across all docs
words_freq <- words %>% 
  count(year, word, sort = TRUE)

# number of total words by doc per year
words_total <- words_freq %>% 
  group_by(year) %>% 
  summarize(total = sum(n))

# join words_freq and words_total 
words_report <- left_join(words_freq, words_total)

# tokenize by paragraphs
paragraph_tokens <- unnest_tokens(tidy_text, 
                                  output = paragraphs, 
                                  input = text, 
                                  token = "paragraphs")
# give each paragraph an id
paragraph_tokens <- paragraph_tokens %>% 
  mutate(par_id = 1:n())
# tokenize by words
paragraph_words <- unnest_tokens(paragraph_tokens,
                                 output = word,
                                 input = paragraphs,
                                 token = "words")

# quanteda word relationship tools
tokens <- tokens(epa_corpus,
                 remove_punct = TRUE)
tokens_1 <- tokens_select(tokens,
                          min_nchar = 3)
tokens_1 <- tokens_tolower(tokens_1)
tokens_1 <- tokens_remove(tokens_1,
                          pattern = (stop_vec))
# create document feature matrix
dfm <- dfm(tokens_1)
tstat_freq <- textstat_frequency(dfm, n = 5, groups = year)
```


1.  What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?

```{r}
tokens_2 <- tokens_ngrams(tokens_1, n = 2)
dfm2 <- dfm(tokens_2)
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n = 20)
freq_words2$token <- rep("bigram", 20)

tstat_freq2 <- textstat_frequency(dfm2, n = 5, groups = year)
head(tstat_freq2, 10)
```


2.  Choose a new focal term to replace "justice" and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!

```{r}

```


3.  Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.


4.  Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create to objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? [Hint](https://tutorials.quanteda.io/advanced-operations/target-word-collocations/) 