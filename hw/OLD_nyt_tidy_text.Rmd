---
title: 'Assignment 1: Text Data in R Using NYT API'
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 5, 
                      echo = TRUE, message = FALSE, warning = FALSE)

librarian::shelf(
  jsonlite,
  tidyverse,
  tidytext,
  ggplot2
)
```


For my assignment, I chose the key word "dinosaur" to query from the NYT API. 

From this API call, I received 10 articles with 33 variables. I also had to convert the object from a list to a data frame. 

```{r, results='hide'}
# create an object t with the results of query ("dinosaur")
t <- fromJSON(paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
                     "dinosaur", 
                     "&api-key=", "mrb3Gx3ed9yP1KDVybtq9F2yEXWIgAga"),
                     flatten = TRUE)

class(t) # t is a list object

# convert t to df
t <- t %>% data.frame()

# inspect data
dim(t) # 10 x 33

# what variables are we working with?
names(t) # 33 total
```

Here I changed the API call to obtain more articles/data. I decided to query based on the dates 2021/01/01 to 2022/04/09, so a little more than one year's worth of data.

```{r, results='hide'}
term <- "dinosaur"
begin_date <- "20210101"
end_date <- "20220409"

# construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=", term,
                  "&begin_date=", begin_date,
                  "&end_date=", end_date,
                  "&facet_filter=true&api-key=","mrb3Gx3ed9yP1KDVybtq9F2yEXWIgAga")

# examine our query url
baseurl
```

Now that I have my query URL, I can obtain the results.

```{r, results='hide', message=FALSE}
# this code allows for obtaining multiple pages of query results 
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10) - 1) # 237 hits is 23 max pages

# pages is an empty list 
pages <- list()
# this for loop.... 
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i + 1]] <- nytSearch 
  Sys.sleep(6) 
}
class(nytSearch)
# dim 7 x 32???

# need to bind the pages and create a tibble from nytDat
nytDat <- rbind_pages(pages)
dim(nytDat) # 237 x 33
```

Now that I have my larger query in the format I want, I can create some visuals. This is a visual of the different types of publications for the key "dinosaur".

```{r}
nytDat %>% 
  group_by(response.docs.type_of_material) %>%
  summarize(count = n()) %>%
  mutate(percent = (count / sum(count)) * 100) %>%
  ggplot() +
  geom_bar(aes(x = response.docs.type_of_material, 
               y = percent,
               fill = response.docs.type_of_material), 
           stat = "identity") + 
  coord_flip()
```

Now that I have my larger query in the format I want, I can create some visuals. This is a visual of the publications per day for the key "dinosaur".

```{r}
nytDat %>%
  mutate(pubDay = gsub("T.*" ,"", response.docs.pub_date)) %>%
  group_by(pubDay) %>%
  summarise(count = n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x = reorder(pubDay, count), 
               y = count), 
           stat="identity") + 
  coord_flip()
```

**TO DO**: Make some (at least 3) transformations to the corpus (add stopword(s), stem a key term and its variants, remove numbers)

To create a word frequency plot using the first paragraph, I first need to tokenize the paragraphs... 

```{r, results='hide'}
paragraph <- names(nytDat)[6] # 6th column, "response.doc.lead_paragraph", is the one we want  
tokenized <- nytDat %>%
  unnest_tokens(word, paragraph)

tokenized[,34]
```

Here I create an initial plot of the tokenized paragraphs. This plot includes words with a frequency of at least 10. The big takeaway from the plot, is that it's not informative because stop words are still included. 

```{r, warning=FALSE}
tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 10) %>% # illegible with all the words displayed
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

Here is the plot again, but without the stop words, and because of that we filtered to a frequency minimum of 5 instead of 10.

```{r, message=FALSE, warning=FALSE}
tokenized <- tokenized %>%
  anti_join(stop_words)

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

Here I am going to recreate the publications per day and the word frequency plots using the headline variable (`response.docs.headline.main`).

```{r, results='hide'}
# tokenizing the headlines
headlines <- names(nytDat)[21] # 21st column, "response.docs.headline.main", is the one we want  
tokenized_hl <- nytDat %>%
  unnest_tokens(output = word, # column to be created
                input = headlines) # column that is split

tokenized_hl[,34]
```


```{r}
# plot with stop words
tokenized_hl %>%
  count(word, sort = TRUE) %>%
  filter(n > 10) %>% # illegible with all the words displayed
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

```{r, message=FALSE}
# plot without stop words
tokenized_hl <- tokenized_hl %>%
  anti_join(stop_words)

tokenized_hl %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


