---
title: "Assignment 4: Sentiment Analysis II"
author: "Halina Do-Linh"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
---

```{r setup, include = FALSE, }
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

# loaded packages 
librarian::shelf(
  quanteda,
  quanteda.sentiment,
  quanteda.textstats,
  qdap, # get top 10 words
  tidyverse,
  tidytext,
  here,
  janitor,
  lubridate,
  wordcloud, # visualization of common words in the data set
  reshape2,
  patchwork
)

# read in data and basic cleaning
ipcc_twitter_data <- read_csv(here("hw/data/IPCC_tweets_April1-10_sample.csv")) %>% 
  clean_names() %>% 
  select(c("date", "title"))
```

First round of tidying: I created the data frame needed to do the sentiment analysis.  

```{r}
ipcc_twitter_clean <- tibble(id = seq(1:length(ipcc_twitter_data$title)),
                             date = as.Date(ipcc_twitter_data$date, "%m/%d/%y"),
                             text = ipcc_twitter_data$title)
```

Here is the cleaning we did from the lab in class.

```{r}
# remove URLs from the tweets
ipcc_twitter_clean$text <- gsub("http[^[:space:]]*", "",
                                ipcc_twitter_clean$text)

# make all the text lowercase 
ipcc_twitter_clean$text <- str_to_lower(ipcc_twitter_clean$text)
```

Remove mentions of twitter accounts from the text field of the tweets tibble.

```{r}
# removing mentions from tweets 
ipcc_twitter_clean$text <- gsub("@\\w+", "", ipcc_twitter_clean$text)
```

Now I'm going to load the sentiment lexicons and tokenize the tweets.

```{r}
# load sentiment lexicons
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')

ipcc_twitter_words <- ipcc_twitter_clean %>%
  select(id, date, text) %>%
  # tokenize tweets to individual words
  unnest_tokens(output = word,
                input = text,
                token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  # remove digits
  mutate(word = str_remove_all(string = word, pattern = "[:digit:]")) %>%
  # remove empty values
  filter(word != "") %>%
  left_join(bing_sent, by = "word") %>%
  left_join(tribble(~ sentiment, ~ sent_score,
                    "positive", 1,
                    "negative",-1),
            by = "sentiment")
```

Before I move forward with sentiment analysis, I create a plot comparing the ten most common terms in the tweets per day. I noticed that some days have more than ten words because the same frequency occurs for those words on a specific day. Additionally, the words "ipcc", "climate", and "report" are the top 3 words for every day except for 2022-04-03 where the second more common word is "dr".

```{r, fig.height=4, fig.width=3}
common_tweets <- ipcc_twitter_words %>% 
  group_by(date) %>% 
  summarize(freq_terms(word, 10))

dates <- unique(common_tweets$date)
plot_list = list()
for (i in seq_along(dates)){

  df <- common_tweets %>%
    filter(date == dates[i])
  
  p <- ggplot(data = df, aes(x = reorder(WORD, FREQ), y = FREQ)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    theme_light() +
    labs(title = paste("Top Words:", dates[[i]]),
       x = NULL,
       y = "Frequency")
  
  plot_list[[i]] = p
  
  print(p)
}
```

\newpage

Here I adjusted the wordcloud from lab so the coloring for positive and negative words are distinguishable. 

```{r, fig.height=3, fig.width=3, fig.align='center'}
ipcc_twitter_words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("darkred", "#d8b365"),
                   max.words = 100)
```

To find the top 10 most tagged accounts in the data set, I first create a corpus using the `quanteda` package.

```{r, results='hide'}
corpus <- corpus(ipcc_twitter_data$title) # enter quanteda
summary(corpus)
```

Then I tokenzie all the words in the corpus (which are tweets).

```{r}
# tokenize the text so each tweet is a list of tokens
tokens <- tokens(corpus)
```

The tokenized words are pretty messy, so I need to do some cleaning before moving forward with my analysis. Here I am removing punctuation (okay to remove because "@" is a symbol), numbers and stop words. 

```{r}
# clean it up
tokens <- tokens(tokens, remove_punct = TRUE,
                 remove_numbers = TRUE)

# stopwords lexicon built in to quanteda
tokens <- tokens_select(tokens, stopwords('english'), selection = 'remove') 

tokens <- tokens_tolower(tokens)
```

After cleaning the corpus, I use `tokens_keep()` to find all the mentions and then create a document-feature matrix (dfm) of the tokens with a tagged account. Then I use `textstat_frequency()` to create a data frame of all the tagged accounts and how frequent they are. 

```{r}
mention_tweets <- tokens(corpus) %>% tokens_keep(pattern = "@*")

dfm_mention <- dfm(mention_tweets)

mention_freq <- textstat_frequency(dfm_mention, n = 100)
head(mention_freq, 10)
```

Lastly, I created a wordcloud of all the mentions.

```{r, fig.height=4, fig.width=4, fig.align='center'}
# tidytext gives us tools to convert to tidy from non-tidy formats
mention_tib <- tidy(dfm_mention)

mention_tib %>%
   count(term) %>%
   with(wordcloud(term, n, max.words = 100))
```

Here I am calling in the "raw tweets" with the sentiment scores calculated by Brandwatch.

```{r}
# read in data and basic cleaning
ipcc_twitter_sent <- read_csv(here("hw/data/IPCC_tweets_April1-10_sample.csv")) %>%
  clean_names() %>%
  select(c("date", "title", "sentiment"))
```

Here I calculated a polarity score and assigned each tweet a polarity score of Positive, Negative, or Neutral.

```{r}
# take average sentiment score by tweet
my_sent <- ipcc_twitter_clean %>%
  left_join(ipcc_twitter_words %>%
              group_by(id) %>%
              summarize(sent_score = mean(sent_score, na.rm = T)),
            by = "id")
# assign scores a category of positive, neutral, or negative
my_sent_classified <- my_sent %>% 
  mutate(sent_score = case_when(sent_score > 0 ~ "positive",
                                sent_score == 0 ~ "neutral",
                                sent_score < 0 ~ "negative")) %>% 
  drop_na(sent_score)
```

I created two plots to show the comparison between the two sentiment classifications.

```{r}
brandwatch_sent <- ipcc_twitter_sent %>% 
  group_by(sentiment) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = reorder(sentiment, count), y = count, 
             fill = sentiment)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_classic() +
  theme(legend.position = "bottom") +
  labs(title = "Brandwatch Sentiment",
       x = "Sentiment",
       y = "Count") +
  geom_text(aes(x = sentiment, y = count,
            label = count),
            nudge_y = 100) +
  scale_fill_manual(name = NULL, 
                    values = c("#d8b365", "lightgray", "#5ab4ac"))

my_sent_plot <- my_sent_classified %>% 
  group_by(sent_score) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = reorder(sent_score, count), y = count,
             fill = sent_score)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_classic() +
  theme(legend.position = "none") +
  labs(title = "My Calculated Sentiment",
       x = "Sentiment",
       y = "Count") +
  geom_text(aes(x = sent_score, y = count,
            label = count),
            nudge_y = 50) +
  scale_fill_manual(name = "Sentiment", 
                    values = c("#d8b365", "lightgray", "#5ab4ac"))

brandwatch_sent / my_sent_plot
```



